---
title: Cassandra. Часть 3. Token ring & vnodes
author: Dmitry Shuplyakov
tags: Cassandra
date: 2017-03-18 22:21:10
---

Cassandra, как и другие NOSQL key-value хранилища, имеет распределенную архитектуру и хранит данные на нескольких серверах. При выполнении записи, БД должна однозначно орпеделять ноду, на которую нужно сохранить данные. Такое же требование касается и чтения, БД, получив ключ документа, должна определить к какой ноде обращаться, чтобы не искать по всем нодам. Поэтому необходим алгоритм выбора нод для чтения/записи, который получивключ записи (key) и вернет адрес ноды. Для того, чтобы этот алгоритм можно было использовать в промышленном решении, определим для него несколько условий:
- *Высокая скорость работы, не меньше 10.000 операций в секунду.*
Операция опеределения ноды очень частая операция, поэтому нельзя в работе подгружать какие-то данные с диска/БД или обращаться по сети в сторонний сервис.
- *Алгоритм должен одинаково работать на всех нодах.*
Запросы на получение данных приходят на все ноды (в materless архитектуре клиент может подключиться к любой ноде), и они самостоятельно должны прийти к верному решению о расположении данных. 
- *При добавлении/удалении нод из кластера не должно быть избыточного перемещения данных.*
При добавлении ноды должны перемещаться только данные, необходимые для ее заполнения. Аналогично, при удалении ноды, только ее данные должны распределиться между остальными нодами. Дополнительных, паразитных перемещений данных не должно быть.

Итак, наша задача описать функцию, которая будет получать ключ и возвращать сервер с расположением документа. 
```F(key) = server```

<!-- more -->

## Hashing
Наиболее очевидный вариант, взять хеш от ключа, разделить на количество серверов и взять остаток от деления. Полученное число использовать как индекс в массиве с адресами серверов:

```servers = [‘server1’, ‘server2’, ‘server3’] ```
```server_for_key(key) = servers[hash(key) % servers.length]```

**Недостатки**
Однако в этом случае, при добавлении/удалении узла потребуется перераспределить большое количество данных. Например, при добавлении в кластер из 4-ех узлов нового узла, 80% данных поменяют свой сервер. См. [Проблемы при работе с кэшем и способы их решения](https://habr.com/ru/company/badoo/blog/352186)


## Сonstistancy hash algorithm
Согласованное хеширование (constistancy hashing) позволяет избежать описанной выше проблемы. Работает оно следующим образом:
1. Определяются фиксированные имена нод, чащего всего это ip-адреса или их hostname.
2. К каждому из них применяется функция хеширования.
3. Полученные хеши помещаются на весь диапазон возможных вариантов хеш функции, разбивая его на интервалы. За каждый интервал отвечает собственная нода. 
4. При поиске узла, от ключа берется хеш, и опеределяется интервал в который он попадает. Так у каждого интервала есть "хозяин", сервер который им владеет, по нему и определяется искомая нода.

**Важно.** Имена нод, на основе которых рассчитывается интервал владения не должны изменяться.


**Перестроение данных.**
При добавлении ноды, хеш ноды попадает на интервал чей-то ноды и разделяет его. Таким образом какая-то нода делится собственными данными, с вновь присоединившейся. 
Удаление ноды запускает обратный процесс, в котором интервалы объединяются.

**Недостатки**
Минус данного подхода в том, что размер интервалов никак не контроллируются, и они могут получится либо очень большие, либо очень маленькие.


## Сonstistancy hash algorithm with virtual nodes
Этот подход использует данные [исследования](https://tom-e-white.com/2007/11/consistent-hashing.html), согласно которому, при увеличении количества серверов распределение будет нормализовываться. 
Для 200 серверов стандартное отклонение составляет около 5%, а для 100 узлов до 10% от среднего.

Идея этого алгоритма заключается в использовании виртуальных нод, вместо реальных серверов. Алгоритм помещения их на интервал хеш-фукнции такой же как в алгоритме согласованного хеширования. Однако дополнительно устанавливается связь между виртуальной нодой и физической нодой, причем физической ноде будет соотетствовать несколько виртуальных. Таким образом, рассчитав на какую виртуальную ноду попадает ключ, с помощью связи определяется физическая нода.


## Token Ring
Все данные, находящиеся в кластере, равномерно распределяются между нодами. Чтобы определить какими данными должна владеть нода, в Кассандре строится кольцо, это кольцо разбивается на интервалы и каждой ноде назначается свой диапазон. Кольцо называется Token ring и представляет собой числовой диапазон. Размер диапазона зависит от алгоритма хеширования, например для алгоритма murmur, он составляет от -2^63 до 2^63.

Во время вставки данных, Кассандра вычисляет хеш от первичного ключа (primary key) и по token ring определяет в какую ноду нужно записывать данные. 

При чтении, Кассандре необходимо выполнить аналогичный процесс, посчитать хеш от первичного ключа и определить с какой ноды читать. Именно по этой причине при конструировании CQL запросов в них обязательно должен быть указан первичный ключ. В противном случае, Кассандре пришлось бы выполнять широковещательный запрос на все ноды, в надежде найти данные на какой-то из них. С точки зрения производительности - это неэффективно, поэтому запросы без указания первичного ключа - запрещены. 

Что будет если одна из нод выйдет из строя? Допустим у нас есть кластер из 6 нод с Replication Factor = 3. 
 - диапазоном данных С владется нода 3, и копии данных лежат на нодах 4 и 5
 - диапазоном данных D владется нода 4, и копии данных лежат на нодах 5 и 6
 - диапазоном данных E владется нода 5, и копии данных лежат на нодах 6 и 1
 
 Если нода 5 откажет, то обрабатывать запросы по ее данным будут ноды 1, 3 и 4. Нода 2 не будет принматься участия в этом, т.к. на ней нет нужных данных. Нода 6 тоже - потому что диапазон данных (Е) уже обрабатывает нода 1. Таким образом, при отказе одной ноды, ее нагрузка распределяется на 3 других ноды. 

![token_ring_in_cassandra.png]({{ site.baseurl }}/images/token_ring_in_cassandra.png)

## Виртуальные ноды (vnodes)

В случае виртуальных нод - каждая нода делит свой интервал на n частей, которые называются виртуальными нодами. Далее виртуальные ноды распределяются случайным образом по всем машинам в кластере. Сколько копий виртуальных нод будет лежать на других машинах определяет Replication Factor. При отказе одной ноды, данные, которыми она владела лежат на всех машинах, поэтому нагрузка распределяются на все ноды. 

Вернувшись к нашему примеру, видно что в схеме в vnod'ами при отказе ноды 5, все остальные ноды будут стримить ее данные. Таким образом подход с vnode'ами позволяет размазать нагрузку по кластеру.

![vnodes_in_cassandra.png]({{ site.baseurl }}/images/vnodes_in_cassandra.png)

Количество данных, которое пересылается по сети в случае token ring  и vnode - одинаковое. 
Основные плюсы vnode:
- при отказе/добавлении ноды, ручная перебалансировка данных не требуется (т.к. нет значительного перекоса по данным)
- при отказе ноды ее данные стримят все ноды кластера

Важно: за число виртуальных нод отвечает параметр num_tokens в файле cassandra.yaml и по умолчанию он равен 256. Такое большое значение неоправдано и влияет на производительность:  растет число SSTable,  дольше выполняется repair, увеличивается использование CPU. Компания thelastpickle.com, специализирующаяся на OpenSource Cassandra рекомендуют установить num_tokens=4. Другая серьезная компания, успешно разрабатывающая собственный форк Кассандры - Datastax, рекомендует устанавливать это число равное 8 (и кстати планирует в будущем сделать его по умолчанию). Изменить количество виртуальных нод в существующем кластере не получится, поэтому задать их число нужно ДО инициализации кластера.

### Ссылки
1. https://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2
2. https://stackoverflow.com/questions/38423888/significance-of-vnodes-in-cassandra
3. http://thelastpickle.com/blog/2019/01/30/new-cluster-recommendations.html
4. https://danielparker.me/cassandra/vnodes/tokens/increasing-vnodes-cassandra/
5. https://www.datastax.com/dev/blog/token-allocation-algorithm
